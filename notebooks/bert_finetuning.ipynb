{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db20461c-fcd4-4d2e-980e-b1bf5ea3485e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.22.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.52.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.80)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cudnn-cu12==9.1.0.70->torch) (12.6.4.1)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.7.77)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.6.1.9->torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch) (59.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.1.30m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.1.3:\u001b[0m \u001b[32m0/5\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.1.0.70━━━\u001b[0m \u001b[32m1/5\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.1.0.70:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70━━━━━\u001b[0m \u001b[32m1/5\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\u001b[0m \u001b[32m2/5\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.1.9:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9━━\u001b[0m \u001b[32m2/5\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.6.090m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.6.0:━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.6.090m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [torch]32m4/5\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 torch-2.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas seaborn matplotlib scikit-learn torch torchvision datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cddf69d-8288-47a4-b8e4-d634f270d22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.4.3)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyterlab in /usr/local/lib/python3.10/dist-packages (from jupyter) (4.4.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.8)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter) (59.6.0)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.2.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.22.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.24.0)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/lib/python3/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.4.1)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.21.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20250516)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets, jupyter-console, jupyter\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [jupyter]m2/5\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.7 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d2e7c-0d3c-4f9c-89d5-bcbc4d2f1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, DataCollatorWithPadding, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43971eb2-74fc-4592-9300-c40ca1be0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "SEED_VALUE = 19\n",
    "random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df72df7-a225-467a-8483-cf53d61490d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/speech_roberta_mlm/checkpoint-600\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c96b9-4a10-4df3-b8a7-f0e2b74eaab7",
   "metadata": {},
   "source": [
    "# ДАТАСЕТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d9d5f57-3e8a-4515-976d-72791c9a2f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tinterviewer\trespondent\tactive-reflexive\n",
      "0\tWhen you are speaking English as a Singaporean, are you aware that you have a certain accent?\tOh yes, hundred percent.\tactive\n",
      "1\t\tI uh I'm aware that I have a slight Singlish accent, I would say.\treflexive\n",
      "2\tOkay.\tLike uh, even when I was in the States my friends told me that I had a little bit of an Asian accent because they didn't know which part I was from.\tactive\n",
      "3\tOkay.\tYeah.\tactive\n",
      "4\tWhen you are speaking English as a Singaporean, are you aware that you have a certain accent?\tUm, I believe that when we speaks English, definitely to us, it sounds very natural. So we don't realize very often that we have a different accent.\tactive\n",
      "5\t\tBut at the same time, I think it's a very common experience when we go overseas and we hear that Singlish accent and the all the additional \"lahs\" and everything that come out, and it's just very distinctive.\tactive\n",
      "6\t\tSo, yes, very aware that we do sound very different, and perhaps this difference does have a certain connotation of value judgment behind it.\treflexive\n",
      "7\tSo do you like your accent?\tI do appreciate my accent. It's one of kind in the whole world. I mean...\tactive\n",
      "8\t\tSinglish has been formed by so many years from our forefathers till now. It's been evolving yeah a lot, and I don't think I would be...\treflexive\n"
     ]
    }
   ],
   "source": [
    "!head ./data/asian_boss.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c57bc244-9cd7-4094-918b-9062edde634d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh yes, hundred percent.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I uh I'm aware that I have a slight Singlish a...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like uh, even when I was in the States my frie...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Um, I believe that when we speaks English, def...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>that's a good question. ah it depends on if yo...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>I don't know, like a million dollars?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>uh yeah, I think it's very possible if you wor...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>first piece of advice, try to come at higher l...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>other, be open to changing the balance of work...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      label\n",
       "0                             Oh yes, hundred percent.     active\n",
       "1    I uh I'm aware that I have a slight Singlish a...  reflexive\n",
       "2    Like uh, even when I was in the States my frie...     active\n",
       "3                                                Yeah.     active\n",
       "4    Um, I believe that when we speaks English, def...     active\n",
       "..                                                 ...        ...\n",
       "132  that's a good question. ah it depends on if yo...  reflexive\n",
       "133              I don't know, like a million dollars?  reflexive\n",
       "134  uh yeah, I think it's very possible if you wor...  reflexive\n",
       "135  first piece of advice, try to come at higher l...  reflexive\n",
       "136  other, be open to changing the balance of work...  reflexive\n",
       "\n",
       "[137 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boss = pd.read_csv('data/asian_boss.tsv', sep='\\t', usecols=[\"respondent\", \"active-reflexive\"])\n",
    "boss.rename(columns={\"respondent\": \"text\", \"active-reflexive\": \"label\"}, inplace=True)\n",
    "boss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6182bf-035f-46de-902e-9a7ea325141f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay. Great. Hi, everyone. Uh, my name is Farh...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, great. Uh, my position is, uh, actually ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The technical, yeah, all the technical issues,...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uh, actually I like aviation. So, and, uh, to ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not my native country, so that's why i...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Uh, sorry?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Uh, a little bit, yeah. A little. Actually, it...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Uzbekistan is native country. I'm as a fish in...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Uh, the main motivation to improve my skills, ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Uh, probably, probably, uh, in Tashkent it was...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Uh, ideal colleague? Interesting question.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The, the first one, uh, uh, - communication. I...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>For example, if I think as like my colleague, ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>So, uh, and also how to explain… former Soviet...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Uh, but in here, when you, uh, operate, uh, no...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>So, and, uh, also technical issues of, uh, air...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yes-yes…</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Uh, I didn't get the… idea</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Uh, the, the biggest problem is customers. As ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sometimes, uh, the, uh, in our case, not custo...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Uh, the best way to tell them, the best way - ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Uh, interesting question. If, uh, probably we ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>If we have some trouble, uh, with my colleague...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How organize?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>By letter or by words? What do you mean?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Communication style?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Just knock the door, open the door, sit and te...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Yes-yes.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Actually, I have two positions in, um, this re...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Uh, and my second position is I am responsible...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The first day I am responsible in Vnukovo airp...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Uh, I come to the airport. I check the, all th...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Yes. Same. Same. But in the office. Mm-hmm. In...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Uh, yes. In aviation, actually my professional...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Uh, it's, uh, actually different, uh, position...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>But before I didn't get that. Uh, so when I st...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>So, and, uh, now I am, I'm not professional, b...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Yes. Yes.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>In professional career? In Russian Federation,...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Uh, to communicate with people. This is the, t...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Before? Yes, because, uh, uh, now I think, in ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>My own business? I never think about that beca...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Um, in which position? You mean? Uh, in which ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Aviation, maybe. May, may, maybe catering. It'...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Because, uh, you know, uh, the business which ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Maybe, that's why. Yes.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Uh, cash back is better.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Where… the catering business? Uh, the best. Yo...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Where? - uh, I think it's Turkey.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Uh, because a lot of flights there.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Where are a lot of flights, it means, uh, you ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>No problem.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>I love it.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Because I speak in English, I remember my Engl...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text      label\n",
       "0   Okay. Great. Hi, everyone. Uh, my name is Farh...  reflexive\n",
       "1   Okay, great. Uh, my position is, uh, actually ...  reflexive\n",
       "2   The technical, yeah, all the technical issues,...     active\n",
       "3   Uh, actually I like aviation. So, and, uh, to ...     active\n",
       "4   This is not my native country, so that's why i...  reflexive\n",
       "5                                          Uh, sorry?  reflexive\n",
       "6   Uh, a little bit, yeah. A little. Actually, it...  reflexive\n",
       "7   Uzbekistan is native country. I'm as a fish in...  reflexive\n",
       "8   Uh, the main motivation to improve my skills, ...     active\n",
       "9   Uh, probably, probably, uh, in Tashkent it was...     active\n",
       "10         Uh, ideal colleague? Interesting question.  reflexive\n",
       "11  The, the first one, uh, uh, - communication. I...  reflexive\n",
       "12  For example, if I think as like my colleague, ...     active\n",
       "13  So, uh, and also how to explain… former Soviet...     active\n",
       "14  Uh, but in here, when you, uh, operate, uh, no...  reflexive\n",
       "15  So, and, uh, also technical issues of, uh, air...     active\n",
       "16                                           Yes-yes…     active\n",
       "17                         Uh, I didn't get the… idea  reflexive\n",
       "18  Uh, the, the biggest problem is customers. As ...  reflexive\n",
       "19  Sometimes, uh, the, uh, in our case, not custo...  reflexive\n",
       "20  Uh, the best way to tell them, the best way - ...  reflexive\n",
       "21  Uh, interesting question. If, uh, probably we ...     active\n",
       "22  If we have some trouble, uh, with my colleague...     active\n",
       "23                                      How organize?  reflexive\n",
       "24           By letter or by words? What do you mean?  reflexive\n",
       "25                               Communication style?  reflexive\n",
       "26  Just knock the door, open the door, sit and te...     active\n",
       "27                                           Yes-yes.     active\n",
       "28  Actually, I have two positions in, um, this re...     active\n",
       "29  Uh, and my second position is I am responsible...     active\n",
       "30  The first day I am responsible in Vnukovo airp...     active\n",
       "31  Uh, I come to the airport. I check the, all th...     active\n",
       "32  Yes. Same. Same. But in the office. Mm-hmm. In...  reflexive\n",
       "33  Uh, yes. In aviation, actually my professional...  reflexive\n",
       "34  Uh, it's, uh, actually different, uh, position...     active\n",
       "35  But before I didn't get that. Uh, so when I st...  reflexive\n",
       "36  So, and, uh, now I am, I'm not professional, b...     active\n",
       "37                                          Yes. Yes.     active\n",
       "38  In professional career? In Russian Federation,...  reflexive\n",
       "39  Uh, to communicate with people. This is the, t...  reflexive\n",
       "40  Before? Yes, because, uh, uh, now I think, in ...  reflexive\n",
       "41  My own business? I never think about that beca...     active\n",
       "42  Um, in which position? You mean? Uh, in which ...  reflexive\n",
       "43  Aviation, maybe. May, may, maybe catering. It'...  reflexive\n",
       "44  Because, uh, you know, uh, the business which ...  reflexive\n",
       "45                            Maybe, that's why. Yes.  reflexive\n",
       "46                           Uh, cash back is better.     active\n",
       "47  Where… the catering business? Uh, the best. Yo...  reflexive\n",
       "48                  Where? - uh, I think it's Turkey.  reflexive\n",
       "49                Uh, because a lot of flights there.     active\n",
       "50  Where are a lot of flights, it means, uh, you ...  reflexive\n",
       "51                                        No problem.     active\n",
       "52                                         I love it.     active\n",
       "53  Because I speak in English, I remember my Engl...     active"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexa = pd.read_csv('data/Alexa_edited.tsv', sep='\\t', usecols=[\"respondent\", \"active-reflexive\"])\n",
    "alexa.rename(columns={\"respondent\": \"text\", \"active-reflexive\": \"label\"}, inplace=True)\n",
    "alexa.at[0, 'label'] = 'reflexive'\n",
    "alexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40dd7d5-3d75-4f20-ae9a-9cd00c12c4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay. So my previous position is the call cent...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And then I would call people and, uh, either t...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I mean, it's the biggest like phone and TV and...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>For example, like, you know, you sometimes mee...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Like for example, you know, yeah, you shouldn'...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>So I think it's kind of, I don't know, mix of ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>But yeah, culture I guess. I mean it's like c...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Ah, that's really nice.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text      label\n",
       "0                                               Okay.  reflexive\n",
       "1                                               Okay.  reflexive\n",
       "2   Okay. So my previous position is the call cent...     active\n",
       "3   And then I would call people and, uh, either t...     active\n",
       "4   I mean, it's the biggest like phone and TV and...     active\n",
       "..                                                ...        ...\n",
       "73  For example, like, you know, you sometimes mee...  reflexive\n",
       "74  Like for example, you know, yeah, you shouldn'...  reflexive\n",
       "75  So I think it's kind of, I don't know, mix of ...  reflexive\n",
       "76   But yeah, culture I guess. I mean it's like c...  reflexive\n",
       "77                            Ah, that's really nice.  reflexive\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maula = pd.read_csv('data/Maula_edited.tsv', sep='\\t', usecols=[\"respondent\", \"active-reflexive\"], skiprows=[1])\n",
    "maula.rename(columns={\"respondent\": \"text\", \"active-reflexive\": \"label\"}, inplace=True)\n",
    "maula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab6d2c7-c78c-40c9-9c51-6664414e68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh yes, hundred percent.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I uh I'm aware that I have a slight Singlish a...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like uh, even when I was in the States my frie...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Um, I believe that when we speaks English, def...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>For example, like, you know, you sometimes mee...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Like for example, you know, yeah, you shouldn'...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>So I think it's kind of, I don't know, mix of ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>But yeah, culture I guess. I mean it's like c...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Ah, that's really nice.</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      label\n",
       "0                             Oh yes, hundred percent.     active\n",
       "1    I uh I'm aware that I have a slight Singlish a...  reflexive\n",
       "2    Like uh, even when I was in the States my frie...     active\n",
       "3                                                Yeah.     active\n",
       "4    Um, I believe that when we speaks English, def...     active\n",
       "..                                                 ...        ...\n",
       "264  For example, like, you know, you sometimes mee...  reflexive\n",
       "265  Like for example, you know, yeah, you shouldn'...  reflexive\n",
       "266  So I think it's kind of, I don't know, mix of ...  reflexive\n",
       "267   But yeah, culture I guess. I mean it's like c...  reflexive\n",
       "268                            Ah, that's really nice.  reflexive\n",
       "\n",
       "[269 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([boss, alexa, maula])\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47af854f-4cc3-427c-bd14-178703b3508c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Moscow Exchange Group of Companies is esse...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And also various instruments for retail, both ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am the head of the department for creating a...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The company has a system in place that there i...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And, accordingly, the second piece is a back-o...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Well, that is, my department is responsible fo...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Also, as an accepted part, we have such an ent...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Well, it certainly depends on the candidate's ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>For example, candidates with such experience, ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Accordingly, expectations from him, it is clea...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The strategy has its own specifics, there is a...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>For example, hypothetically, if an employee ca...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Do you mean format?</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Well, the format is built in such a way that i...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Accordingly, from my side, my employees, from ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>And, accordingly, from the management side, if...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What I like about my work is that it's like we...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yes, a top-down view of many processes that ev...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Of course, this creates some difficulties, yes...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>We must also look at international experience ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>And in general, without understanding where th...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>But we are forced to understand, as it were, w...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Accordingly, I like the fact that in our work ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Initially, the desire to develop in financial ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>As with any strategy, we have some, let's call...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>This, accordingly, goal, it is decomposed into...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Achievement. Hmm, just the other day, last wee...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The work environment... Well, the ideal work e...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>That is, as it were, of course, in general ter...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yes, it can vary greatly from day to day. But ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It is difficult, of course, to say, considerin...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>At some point, for example, in the summer, we ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Considering that the work is extremely heterog...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text      label\n",
       "0   The Moscow Exchange Group of Companies is esse...  reflexive\n",
       "1   And also various instruments for retail, both ...     active\n",
       "2   I am the head of the department for creating a...     active\n",
       "3   The company has a system in place that there i...     active\n",
       "4   And, accordingly, the second piece is a back-o...  reflexive\n",
       "5   Well, that is, my department is responsible fo...     active\n",
       "6   Also, as an accepted part, we have such an ent...     active\n",
       "7   Well, it certainly depends on the candidate's ...  reflexive\n",
       "8   For example, candidates with such experience, ...     active\n",
       "9   Accordingly, expectations from him, it is clea...  reflexive\n",
       "10  The strategy has its own specifics, there is a...     active\n",
       "11  For example, hypothetically, if an employee ca...  reflexive\n",
       "12                                Do you mean format?  reflexive\n",
       "13  Well, the format is built in such a way that i...     active\n",
       "14  Accordingly, from my side, my employees, from ...     active\n",
       "15  And, accordingly, from the management side, if...     active\n",
       "16  What I like about my work is that it's like we...  reflexive\n",
       "17  Yes, a top-down view of many processes that ev...     active\n",
       "18  Of course, this creates some difficulties, yes...     active\n",
       "19  We must also look at international experience ...  reflexive\n",
       "20  And in general, without understanding where th...  reflexive\n",
       "21  But we are forced to understand, as it were, w...     active\n",
       "22  Accordingly, I like the fact that in our work ...     active\n",
       "23  Initially, the desire to develop in financial ...     active\n",
       "24  As with any strategy, we have some, let's call...     active\n",
       "25  This, accordingly, goal, it is decomposed into...     active\n",
       "26  Achievement. Hmm, just the other day, last wee...     active\n",
       "27  The work environment... Well, the ideal work e...  reflexive\n",
       "28  That is, as it were, of course, in general ter...     active\n",
       "29  Yes, it can vary greatly from day to day. But ...  reflexive\n",
       "30  It is difficult, of course, to say, considerin...     active\n",
       "31  At some point, for example, in the summer, we ...     active\n",
       "32  Considering that the work is extremely heterog...  reflexive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marfenko = pd.read_csv('data/Marfenko_edited.tsv', sep='\\t', usecols=[\"respondent\", \"active-reflexive\"])\n",
    "marfenko.rename(columns={\"respondent\": \"text\", \"active-reflexive\": \"label\"}, inplace=True)\n",
    "marfenko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15937732-427b-45d8-8211-33548c43aec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh yes, hundred percent.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I uh I'm aware that I have a slight Singlish a...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Like uh, even when I was in the States my frie...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yeah.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Um, I believe that when we speaks English, def...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>That is, as it were, of course, in general ter...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Yes, it can vary greatly from day to day. But ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>It is difficult, of course, to say, considerin...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>At some point, for example, in the summer, we ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Considering that the work is extremely heterog...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      label\n",
       "0                             Oh yes, hundred percent.     active\n",
       "1    I uh I'm aware that I have a slight Singlish a...  reflexive\n",
       "2    Like uh, even when I was in the States my frie...     active\n",
       "3                                                Yeah.     active\n",
       "4    Um, I believe that when we speaks English, def...     active\n",
       "..                                                 ...        ...\n",
       "297  That is, as it were, of course, in general ter...     active\n",
       "298  Yes, it can vary greatly from day to day. But ...  reflexive\n",
       "299  It is difficult, of course, to say, considerin...     active\n",
       "300  At some point, for example, in the summer, we ...     active\n",
       "301  Considering that the work is extremely heterog...  reflexive\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, marfenko])\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0716efb2-6d70-4185-a390-4613705b1438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh yes, hundred percent.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i uh i'm aware that i have a slight singlish a...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like uh, even when i was in the states my frie...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeah.</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>um, i believe that when we speaks english, def...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>that is, as it were, of course, in general ter...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>yes, it can vary greatly from day to day. but ...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>it is difficult, of course, to say, considerin...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>at some point, for example, in the summer, we ...</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>considering that the work is extremely heterog...</td>\n",
       "      <td>reflexive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text      label\n",
       "0                             oh yes, hundred percent.     active\n",
       "1    i uh i'm aware that i have a slight singlish a...  reflexive\n",
       "2    like uh, even when i was in the states my frie...     active\n",
       "3                                                yeah.     active\n",
       "4    um, i believe that when we speaks english, def...     active\n",
       "..                                                 ...        ...\n",
       "297  that is, as it were, of course, in general ter...     active\n",
       "298  yes, it can vary greatly from day to day. but ...  reflexive\n",
       "299  it is difficult, of course, to say, considerin...     active\n",
       "300  at some point, for example, in the summer, we ...     active\n",
       "301  considering that the work is extremely heterog...  reflexive\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac646840-8a70-4411-bb76-9c9b0a9585a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1505/1644994870.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data.replace('reflexive', 0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh yes, hundred percent.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i uh i'm aware that i have a slight singlish a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like uh, even when i was in the states my frie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeah.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>um, i believe that when we speaks english, def...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>that is, as it were, of course, in general ter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>yes, it can vary greatly from day to day. but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>it is difficult, of course, to say, considerin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>at some point, for example, in the summer, we ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>considering that the work is extremely heterog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0                             oh yes, hundred percent.      1\n",
       "1    i uh i'm aware that i have a slight singlish a...      0\n",
       "2    like uh, even when i was in the states my frie...      1\n",
       "3                                                yeah.      1\n",
       "4    um, i believe that when we speaks english, def...      1\n",
       "..                                                 ...    ...\n",
       "297  that is, as it were, of course, in general ter...      1\n",
       "298  yes, it can vary greatly from day to day. but ...      0\n",
       "299  it is difficult, of course, to say, considerin...      1\n",
       "300  at some point, for example, in the summer, we ...      1\n",
       "301  considering that the work is extremely heterog...      0\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace('active', 1, inplace=True)\n",
    "data.replace('reflexive', 0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c7b2398-372b-4042-98ef-6567680edcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0aef32-41db-409f-8d45-7f67fb4e449b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'oh yes, hundred percent.', 'label': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0e266-6b79-4e4d-8fda-ba594f979d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "675c82f2-2107-428f-a2ef-a2116dc3af31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 241\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 31\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
    "train_testvalid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "train_test_valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7553794e-8163-41b9-9ec7-1e35fa061821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1484e18c0f47e5a95a56d193a7d63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/241 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e322eea8214657978cf0efd58a9e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb00c30d6b2d4b2e92b29f32be194384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        #padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_tokenised = train_test_valid_dataset['train'].map(tokenize_text, batched=True, num_proc=8)\n",
    "val_tokenised = train_test_valid_dataset['valid'].map(tokenize_text, batched=True, num_proc=8)\n",
    "test_tokenised = train_test_valid_dataset['test'].map(tokenize_text, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c24ebc-1e63-44e5-a6c3-06826449bc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"i mean, it's the biggest like phone and tv and like network operator in my country. so, um, they have call centers. they do a lot of business with the government and like when i was working there, there was different this like, um, section. so i was, uh, on the outgoing calls. \",\n",
       " 'label': 1,\n",
       " 'input_ids': [0,\n",
       "  118,\n",
       "  1266,\n",
       "  6,\n",
       "  24,\n",
       "  18,\n",
       "  5,\n",
       "  934,\n",
       "  101,\n",
       "  1028,\n",
       "  8,\n",
       "  30016,\n",
       "  8,\n",
       "  101,\n",
       "  1546,\n",
       "  5364,\n",
       "  11,\n",
       "  127,\n",
       "  247,\n",
       "  4,\n",
       "  98,\n",
       "  6,\n",
       "  7252,\n",
       "  6,\n",
       "  51,\n",
       "  33,\n",
       "  486,\n",
       "  5228,\n",
       "  4,\n",
       "  51,\n",
       "  109,\n",
       "  10,\n",
       "  319,\n",
       "  9,\n",
       "  265,\n",
       "  19,\n",
       "  5,\n",
       "  168,\n",
       "  8,\n",
       "  101,\n",
       "  77,\n",
       "  939,\n",
       "  21,\n",
       "  447,\n",
       "  89,\n",
       "  6,\n",
       "  89,\n",
       "  21,\n",
       "  430,\n",
       "  42,\n",
       "  101,\n",
       "  6,\n",
       "  7252,\n",
       "  6,\n",
       "  2810,\n",
       "  4,\n",
       "  98,\n",
       "  939,\n",
       "  21,\n",
       "  6,\n",
       "  37463,\n",
       "  6,\n",
       "  15,\n",
       "  5,\n",
       "  14234,\n",
       "  1519,\n",
       "  4,\n",
       "  1437,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenised[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfe4c45-fda1-400b-86d8-93c0ad25f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a6873-47a7-4783-8a9b-42dedac8145e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8e4c9-5f93-46c5-9949-86045a65289c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d576be-a387-4b5f-b765-18d621695404",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a2d5df-8031-4b8d-8721-f3b358da4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"reflexive\", 1: \"active\"}\n",
    "label2id = {\"reflexive\": 0, \"active\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b64834-3b70-4e6a-add6-da5e89b3f589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at checkpoints/speech_roberta_mlm/checkpoint-600 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_classifier = RobertaForSequenceClassification.from_pretrained(\"checkpoints/speech_roberta_mlm/checkpoint-600\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e63679-8957-4518-ba9f-0690e4f0dd61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b41435dd-ebce-4267-a26a-260b268704c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (5.4.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (1.1.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate>=0.26.0) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f19e728e-144f-4d1f-92bc-4fc48459a4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers[torch]) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Collecting torch<2.7,>=2.1 (from transformers[torch])\n",
      "  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<2.7,>=2.1->transformers[torch])\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.3.1\n",
      "\u001b[2K    Uninstalling triton-3.3.1:\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12━━\u001b[0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.30m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu1290m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.1[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.22.1 requires torch==2.7.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb76403-d61f-41a0-8963-d9b5db547f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/speech_roberta_classification\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=200,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "\n",
    "    fp16=True,\n",
    "    logging_dir=\"logs/finetune\",\n",
    "\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e6b7d8-154a-4d8c-8d98-7c5dc7f15834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cad6ea2-9c28-42bb-aa49-8e8fbcce4507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 52:53, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764103</td>\n",
       "      <td>0.739234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>4.833129</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.693056</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.673324</td>\n",
       "      <td>0.629187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.13202063389122487, metrics={'train_runtime': 3190.5802, 'train_samples_per_second': 15.107, 'train_steps_per_second': 0.063, 'total_flos': 3891738828149700.0, 'train_loss': 0.13202063389122487, 'epoch': 200.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=roberta_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenised,\n",
    "    eval_dataset=val_tokenised,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ceb6f33-45e3-40ae-b3b4-47e0ad0ad168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/speech_roberta_classification/checkpoint-100'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72138d34-31cf-4e99-83bc-e06b6950e2b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/speech_roberta_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# размеры батчей и оптимизатор\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, \u001b[38;5;66;03m# ??????\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      8\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      9\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     10\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     12\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# валидация/логирование/чекпоинты\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#evaluation_strategy=\"epoch\",\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;66;03m# было 300\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     19\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     20\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;66;03m# было 300\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# загрузить лучшую модель после обучения\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/finetune\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m     optim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madamw_torch_fused\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     dataloader_pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     dataloader_num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     32\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStoppingCallback(\n\u001b[1;32m     37\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mroberta_classifier,\n\u001b[1;32m     43\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     50\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "   training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/speech_roberta_classification\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=300,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "\n",
    "    fp16=True,\n",
    "    logging_dir=\"logs/finetune\",\n",
    "\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=roberta_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenised,\n",
    "    eval_dataset=val_tokenised,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19b7e4d1-8194-452f-b17d-c083905b1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_classifier_og = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5475a647-cc98-4e9a-9b6c-d8b227744737",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"checkpoints/speech_roberta_classification_og\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=200,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "\n",
    "    fp16=True,\n",
    "    logging_dir=\"logs/finetune\",\n",
    "\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5a8729e-87cb-4d3d-a98e-b620f2a43921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 52:54, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.639785</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.781609</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>1.141300</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.730021</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.716402</td>\n",
       "      <td>0.674641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>3.742014</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.693056</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.673324</td>\n",
       "      <td>0.629187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.910664</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.650104</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.645503</td>\n",
       "      <td>0.602871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.12874172754350638, metrics={'train_runtime': 3189.1535, 'train_samples_per_second': 15.114, 'train_steps_per_second': 0.063, 'total_flos': 3891738828149700.0, 'train_loss': 0.12874172754350638, 'epoch': 200.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=roberta_classifier_og,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_tokenised,\n",
    "    eval_dataset=val_tokenised,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed28a42-4f63-4daf-9b49-bc40bcfa211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    171\n",
       "1    131\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0020108d-a7b2-45bb-8c81-9c1a172faa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAHDCAYAAADWXDiiAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVj1JREFUeJzt3X1cjff/B/DX6T6lk6ISUXNXaOR2uQ2RmLtsaJncfDETk9vZltsRZu4Ztim2mi2Mja3N7YwlRI0JIctQSauUdaPz+f3h0fVznOKcnDp1vJ6Px3k8nM/1uT7X+1yuc15dd+fIhBACREREpBcMdF0AERERaQ+DnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8Y6boAIiKiqqawsBCZmZlQKBRwdHTUdTka4R47ERHprf379yM+Pl56vnfvXvz111+l9j179izeeust1K5dG6ampqhbty6GDh1aSZVqT6UGe3h4OGQymfQwMzND06ZNERQUhLS0tMoshYiIXgIXLlzAe++9h6SkJJw6dQrvvPMOHjx4oNJv37596NKlCy5duoQlS5bg4MGDOHjwILZs2aKDql+MrDK/Kz48PBxjxozBokWL4OLigvz8fJw4cQJfffUVGjZsiIsXL6JGjRqVVQ4REem5e/fuoVOnTrh27RoAwM/PD7t371bqk5mZiWbNmqFTp06IioqCiYmJLkrVGp2cY/f19UW7du0AAP/73/9ga2uLVatWYd++ffD399dFSUREpIfq1KmDixcvSjuObm5uKn3CwsKQn5+P8PDwah/qQBU5x96zZ08AQHJyMoDHfz3NnDkT7u7usLS0hJWVFXx9fZGQkKAyb35+PhYsWICmTZvCzMwMdevWhZ+fH65fvw4AuHnzptLh/6cfXl5e0ljHjh2DTCbDt99+iw8++AAODg6wsLDAwIEDcevWLZVlx8bGom/fvpDL5ahRowa6d++OkydPlvoavby8Sl3+ggULVPp+/fXXaNu2LczNzWFjY4MRI0aUuvxnvbYnKRQKrFmzBi1atICZmRns7e0xceJE/Pvvv0r9nJ2d8frrr6ssJygoSGXM0mr/5JNPVNYpABQUFGD+/Plo3LgxTE1N4eTkhNmzZ6OgoKDUdfWk33//HW+++SYaNGggzRscHIz//vtPqd/o0aNhaWmpMv+uXbsgk8lw7Ngxqc3LywstW7ZU6bty5UrIZDLcvHlTqX3Tpk1o0aIFTE1N4ejoiMmTJyMrK0tpvGdtY0+uO3XXm6bbYlRUlLTN1K5dGyNHjsTt27dV1tGTNdWqVQteXl74/ffflfrt27cP/fv3h6OjI0xNTdGoUSMsXrwYxcXFSv00WY+abltBQUEqfUuUnNIrGf/IkSMwMDDAvHnzlPpFRkZCJpPhs88+K3Osp5db2uPJbaewsBDz5s1D27ZtIZfLYWFhga5du+Lo0aNKY5W8N1euXKmynJYtW5b6f71r1y6VvpaWlhg9erTKaz979myZr8PLy0tp/MDAQJiZmSExMVGpn4+PD2rVqoU7d+6UOZYmn58AkJ6ejnHjxsHe3h5mZmZo1aoVtm/fXub4TytZF08/nJ2dlfqp+5lS8n4zNTVF27Zt4ebmVur77dSpU2jdujWWLl0KJycnmJqaokmTJli2bBkUCoVKnU+fVi5tfaibY0++5ievBQCA27dvw9DQsMztoyxV4qr4khC2tbUFANy4cQN79+7Fm2++CRcXF6SlpWHLli3o3r07Ll26JF2hWFxcjNdffx2HDx/GiBEj8N577+HBgwc4ePAgLl68iEaNGknL8Pf3R79+/ZSWO3fu3FLrWbJkCWQyGebMmYP09HSsWbMG3t7eiI+Ph7m5OYDHHya+vr5o27Yt5s+fDwMDA4SFhaFnz574/fff0aFDB5Vx69evj9DQUABAbm4uJk2aVOqyQ0JCMGzYMPzvf//DvXv3sH79enTr1g3nz5+HtbW1yjwTJkxA165dAQB79uzB999/rzR94sSJ0mmQqVOnIjk5GRs2bMD58+dx8uRJGBsbl7oeNJGVlSW9ticpFAoMHDgQJ06cwIQJE+Dm5oYLFy5g9erVuHr1Kvbu3fvMcaOiovDw4UNMmjQJtra2OH36NNavX49//vkHUVFRL1z38yxYsAALFy6Et7c3Jk2ahCtXruCzzz7DmTNnpHX34Ycf4n//+x8AICMjA8HBwUr/J89S1noroc62WPJ/2759e4SGhiItLQ1r167FyZMnVbaZ2rVrY/Xq1QCAf/75B2vXrkW/fv1w69YtqV94eDgsLS0xffp0WFpa4siRI5g3bx5ycnLwySeflHNNVpyePXvi3XffRWhoKAYPHow2bdrg7t27mDJlCry9vfHOO++oPVbv3r0xatQoAMCZM2ewbt06pek5OTn44osv4O/vj/Hjx+PBgwf48ssv4ePjg9OnT6N169bafGlasXbtWhw5cgSBgYGIiYmBoaEhtmzZgl9//RVfffWVWld8q/P5+d9//8HLywvXrl1DUFAQXFxcEBUVhdGjRyMrKwvvvfee2jV/8MEH0p711q1bkZKSIk17kc+Ust5v9+/fx4kTJ3DixAmMHTsWbdu2xeHDhzF37lzcvHkTmzdvLnW81atXo3bt2gAev1efpG6OlTAzM0NYWBjWrl0rtW3fvh0mJibIz89/9gp7mqhEYWFhAoA4dOiQuHfvnrh165bYuXOnsLW1Febm5uKff/4RQgiRn58viouLleZNTk4WpqamYtGiRVLbtm3bBACxatUqlWUpFAppPgDik08+UenTokUL0b17d+n50aNHBQBRr149kZOTI7V/9913AoBYu3atNHaTJk2Ej4+PtBwhhHj48KFwcXERvXv3VllWp06dRMuWLaXn9+7dEwDE/PnzpbabN28KQ0NDsWTJEqV5L1y4IIyMjFTak5KSBACxfft2qW3+/Pniyf/W33//XQAQERERSvNGR0ertDds2FD0799fpfbJkyeLpzeVp2ufPXu2sLOzE23btlVap1999ZUwMDAQv//+u9L8mzdvFgDEyZMnVZb3pIcPH6q0hYaGCplMJv7++2+pLTAwUFhYWKj0jYqKEgDE0aNHpbbu3buLFi1aqPT95JNPBACRnJwshBAiPT1dmJiYiD59+ihtjxs2bBAAxLZt21TGKNnewsLCSn096q43dbfFwsJCYWdnJ1q2bCn+++8/qd/+/fsFADFv3jylddSwYUOlerZu3SoAiNOnT0ttpa3ziRMniho1aoj8/HypTd31KITm29bkyZNV+pYo+Rx5cvy8vDzRuHFj0aJFC5Gfny/69+8vrKyslLaRZyksLBQARFBQkNRW2rbz6NEjUVBQoDTvv//+K+zt7cXYsWOltvJ87kRFRan0tbCwEIGBgSqv/cyZM2W+lu7duyuNL4QQv/zyiwAgPv74Y3Hjxg1haWkpBg8eXOYY5Xkda9asEQDE119/LbUVFhYKT09PYWlpqbQdl+XgwYMCgPjtt9+ktqe3W00+U9R9v3Xv3l0AEAsWLFAac/To0QKAuHDhglL7559/LgAobV9Pr3d1c6zk/9/f31/Y2toqbV9NmjQRb731VpnbR1l0cije29sbderUgZOTE0aMGAFLS0t8//33qFevHgDA1NQUBgaPSysuLsb9+/dhaWmJZs2a4dy5c9I4u3fvRu3atTFlyhSVZTx9eE8To0aNQs2aNaXnb7zxBurWrYuffvoJABAfH4+kpCS89dZbuH//PjIyMpCRkYG8vDz06tULx48fVzl8k5+fDzMzs2cud8+ePVAoFBg2bJg0ZkZGBhwcHNCkSROVw32FhYUAHq+vskRFRUEul6N3795KY7Zt2xaWlpYqYxYVFSn1y8jIeO5fi7dv38b69esREhKicjg8KioKbm5ucHV1VRqz5PTL08t/WsleKQDk5eUhIyMDnTp1ghAC58+fV+n/dO2lXf0KPN6unu778OFDpT6HDh1CYWEhpk2bJm2PADB+/HhYWVnhwIEDz6z9eZ613ko8b1s8e/Ys0tPT8e677yptX/3794erq6tKjQqFQnq98fHx2LFjB+rWrat03vHJdf7gwQNkZGSga9euePjwIS5fvqw0njrrsYQm21Z+fj4yMjJw//79Ug+FPq1GjRoIDw9HYmIiunXrhgMHDmD16tVo0KDBc+ctWR6A575HDQ0NpXOwCoUCmZmZePToEdq1a6f02VTi4cOHKq/56VMaJUrW9ZOPsmRnZz9z+35anz59MHHiRCxatAh+fn4wMzPT+tXeP/30ExwcHJSukzI2NsbUqVORm5uL33777bljqPuZVp7PlOe93wwNDREcHKzUNmPGDABQeR+pU6e6OVZiwIABkMlk+OGHHwA8Pg35zz//YPjw4WUuoyw6ORS/ceNGNG3aFEZGRrC3t0ezZs2UPjgVCgXWrl2LTZs2ITk5WemNUHK4Hnh8CL9Zs2YwMtLuy2jSpInSc5lMhsaNG0vn9JKSkgA8PndVluzsbNSqVUt6npGRoTLu05KSkiCEKLPf04fMS87zlhUKJWNmZ2fDzs6u1Onp6elKz3/99VfUqVPnmXU+bf78+XB0dMTEiRNVzgMlJSUhMTGxzDGfXv7TUlJSMG/ePPzwww8q1wRkZ2crPc/Ly1O79suXLz+3799//w0AaNasmVK7iYkJXnnlFWl6eT1rvZV43rZYVo0A4OrqihMnTii13bp1S+l1161bF7t371bahv766y989NFHOHLkCHJycpTmf3qdq7MeS2iybX355Zf48ssvATxe3x07dsSqVauki25L07lzZ0yaNAkbN26Ej48Pxo4dq9ayAEghKpfLn9t3+/bt+PTTT3H58mUUFRVJ7S4uLip958+fj/nz56u029vbq7RpUq+3t7f0b2tra/j7++OTTz6BhYVFmfOsXLkS+/btQ3x8PCIjI8v8TCivv//+G02aNFH6LAcg/dGozvtF3c+08nymPOv9JpPJ4OjoCCsrK6X2kmx6+robdepUN8dKGBsbY+TIkdi2bRveeOMNbNu2DUOHDlWpSR06CfYOHTo88w26dOlShISEYOzYsVi8eDFsbGxgYGCAadOmqfXXe0UrqeGTTz4p85zak//hhYWFuHv3Lnr37v3ccWUyGX7++WcYGho+c0wASE1NBQA4ODg8c0w7OztERESUOv3pN0fHjh3x8ccfK7Vt2LAB+/btK3X+xMREhIeH4+uvvy71XL1CoYC7uztWrVpV6vxOTk5l1l5cXIzevXsjMzMTc+bMgaurKywsLHD79m2MHj1aZVswMzPDjz/+qNT2+++/Y9GiRSpjOzs74/PPP1dqi4qKwtatW8usR5uet94qir29Pb7++msAj0N627Zt6Nu3L06cOAF3d3dkZWWhe/fusLKywqJFi9CoUSOYmZnh3LlzmDNnjso612Q9arJtDRo0CEFBQRBCIDk5GYsWLcLrr78u/VFdmoKCAulCt+vXr+Phw4dq3z5b8sH99EVaT/v6668xevRoDB48GLNmzYKdnR0MDQ0RGhoqXSv0pAkTJuDNN99Uahs/fnypY8+bN0/luowBAwaU2rdk56jkNZdcpLdp06Yyaz9//rwUehcuXKiSdyCp+5mm6WfK895vTx6lUrdOS0vLZ/4hVZ4cGzt2LDw8PHDlyhVERUVJe++aqhIXzz1t165d6NGjh/QXe4msrCzpQgUAaNSoEWJjY1FUVKTVD8enPzyEELh27RpeffVVabkAYGVlpfSXc1kSEhJQVFT0zD9mSsYVQsDFxQVNmzZ97riXLl2CTCYrdW/tyTEPHTqEzp07q7Xx1q5dW+U1PetilLlz56J169ZlHi5q1KgREhIS0KtXL41Pj1y4cAFXr17F9u3bpQuaAODgwYOl9jc0NFSp/cmr159kYWGh0vfpK1IbNmwIALhy5QpeeeUVqb2wsBDJyclq/d+X5XnrrcTztsUnayw5FFniypUr0vQSZmZmSnUPHDgQNjY22LBhA7Zs2YJjx47h/v372LNnD7p16yb1K7lj5WnqrMcSmmxb9evXV+praWmJgICAUk+/lJg/fz4SExOxcuVKzJkzB++//77KxW9lKbnK/Hnv0V27duGVV17Bnj17lLbn0vbKgcdHXJ5+zWWFgbu7u0rf0v7AB5R3jvr374+EhARER0eXWXdeXh7GjBmD5s2bo1OnTlixYgWGDBmC9u3blzmPpho2bIg///wTCoVCaa+95PTN09tiaS5duoQ6deqUukdbojyfKc97v7m4uODXX3/FgwcPlE59Xb16FQqFQuUPvkuXLpV629yT1M2xJ7m7u8PDwwPDhg1DnTp10KNHD7VOYTytStzu9jRDQ0OIp743JyoqSuX2naFDhyIjIwMbNmxQGePp+TWxY8cOpXNXu3btwt27d+Hr6wsAaNu2LRo1aoSVK1ciNzdXZf579+6p1G5oaFjq7T5P8vPzg6GhIRYuXKhSvxAC9+/fl54/evQIu3fvRocOHZ55OGjYsGEoLi7G4sWLVaY9evSozOBTR0xMDPbt24dly5aV+QYbNmwYbt++rbJXBzy+ijYvL6/M8Us+1J5cF0IIpatGK5K3tzdMTEywbt06pRq+/PJLZGdno3///uUaV531VuJ522K7du1gZ2eHzZs3K93q8/PPPyMxMfG5NRYWFuLRo0fSvKWt88LCwmfuCVaGkj2csoIuNjYWK1euxLRp0zBjxgzMmjULGzZsUPtDcdeuXWjWrBlcXV2f2a+09RMbG4uYmBi1llNRFApFmesGAObMmYOUlBRs374dq1atgrOzMwIDA9W65VRd/fr1Q2pqKr799lup7dGjR1i/fj0sLS3RvXv3Z87/4MED/PTTTyp/oD5N088Udd5v/fr1Q3FxsUqWlBwVePJ9dOvWLZw8efK5daqbY08bO3Ys/vzzT+n21PKoknvsr7/+OhYtWoQxY8agU6dOuHDhAiIiIpT2moDHFxbt2LED06dPx+nTp9G1a1fk5eXh0KFDePfddzFo0KByLd/GxgZdunTBmDFjkJaWhjVr1qBx48bSITQDAwN88cUX8PX1RYsWLTBmzBjUq1cPt2/fxtGjR2FlZYUff/wReXl52LhxI9atW4emTZsq3Q9b8gfBn3/+iZiYGHh6eqJRo0b4+OOPpVssBg8ejJo1ayI5ORnff/89JkyYgJkzZ+LQoUMICQnBn3/+qXLo+Wndu3fHxIkTERoaivj4ePTp0wfGxsZISkpCVFQU1q5dizfeeKNc6+nXX39F7969n7nn+vbbb+O7777DO++8g6NHj6Jz584oLi7G5cuX8d133+GXX34pcy/J1dUVjRo1wsyZM3H79m1YWVlh9+7dKufaK0qdOnUwd+5cLFy4EH379sXAgQNx5coVbNq0Ce3bt8fIkSPLNa46663E87ZFY2NjLF++HGPGjEH37t3h7+8v3e7m7OyscjFQXl6e0qH4r776Cvn5+RgyZAgAoFOnTqhVqxYCAwMxdepUyGQyfPXVVy/0h3J5pKSkIDo6WjoUv2TJEjRs2BAeHh4qRzHy8/MRGBiIJk2aSLccLVy4ED/++CPGjBmDCxculLmXfOPGDaxYsQKnT5+Gn5+ftG6Ax7e7AY+PEDVo0ACvvPIKXn/9dezZswdDhgxB//79kZycjM2bN6N58+al/pFfUWJiYpCRkSEdij98+DBmzpxZat8jR45g06ZNmD9/Ptq0aQPg8ReyeHl5ISQkBCtWrNBKTRMmTMCWLVswevRoxMXFwdnZGbt27cLJkyexZs0apT3hp3333XdYuHAh/v33X7z//vvPXI6mnynqvN/69esHb29vfPjhh0hOTkbr1q1x5MgR7N69G++88470fQ2fffYZQkNDUaNGDUydOvWZdaqbY08bP3483nzzTbWu9yiT2tfPa4E6t2oI8fg2gRkzZoi6desKc3Nz0blzZxETE1PqbRwPHz4UH374oXBxcRHGxsbCwcFBvPHGG+L69etCiPLddvLNN9+IuXPnCjs7O2Fubi769+9f6m0z58+fF35+fsLW1laYmpqKhg0bimHDhonDhw8rLft5jydvZxFCiN27d4suXboICwsLYWFhIVxdXcXkyZPFlStXhBBCTJkyRXTr1k1ER0er1PT07W4ltm7dKtq2bSvMzc1FzZo1hbu7u5g9e7a4c+eO1EfTW5JkMpmIi4tTai/t/6iwsFAsX75ctGjRQpiamopatWqJtm3bioULF4rs7GyV5T3p0qVLwtvbW1haWoratWuL8ePHi4SEBJVbyiridrcSGzZsEK6ursLY2FjY29uLSZMmiX///bfUetW53U2d9abptvjtt98KDw8PYWpqKmxsbERAQIB0+2iJwMBApe3O0tJStGnTRnz11VdK/U6ePClee+01YW5uLhwdHcXs2bOl26XKux413bZKHjKZTDg4OAg/Pz+RmJgohFC93S04OFgYGhqK2NhYpXHOnj0rjIyMxKRJk1SWW6JkrOc9Sv4/FQqFWLp0qWjYsKEwNTUVHh4eYv/+/Sq3ZFX07W4lDxMTE9G4cWMxb9486TapJ7elnJwc0bBhQ9GmTRtRVFSkNHZwcLAwMDAQMTExZa4fTV6HEEKkpaWJMWPGiNq1awsTExPh7u5e5nvhSUOGDBG+vr4q/4dClH6bprqfKZp8TuXm5org4GDh6OgojI2NRePGjcWyZcuUblnr0KGDePPNN8Xly5dV6iztdjd1cuxZ///qTC9NpX5XfFV37Ngx9OjRA1FRUeXei33SzZs34eLiguTk5DIvylmwYAFu3ryJ8PDwF14e6Q9tb4tUuvDwcOk9WBYvLy+MHj1a6RvgiKqyKnmOnYiIiMqnSp5j1xclV/I+6+K2V199Va2vdCQi7WvUqJF0fUFZevfurfT11ERVHYO9AtWuXVvpYpzS+Pn5VVI1RPS0rl27Pvc7/T/88MNKqoZIO3iOnYiISI/wHDsREZEeYbATERHpEZ5jx+Nvbbpz5w5q1qz5Qr8KR0RE1ZcQAg8ePICjo6PKj9lUJwx2AHfu3Hnmj5EQEdHL49atW6hfv76uyyg3BjsgfdXhrVu3yvUTeUREVP3l5OTAycnpmV9/Wx0w2AHp8LuVlRWDnYjoJVfdT8lW35MIREREpILBTkREpEcY7ERERHqE59g1UFxcjKKiIl2XQZXI2NgYhoaGui6DiEhtDHY1CCGQmpqKrKwsXZdCOmBtbQ0HB4dqf0ENEb0cGOxqKAl1Ozs71KhRgx/wLwkhBB4+fIj09HQAQN26dXVcERHR8zHYn6O4uFgKdVtbW12XQ5XM3NwcAJCeng47OzselieiKo8Xzz1HyTn1GjVq6LgS0pWS/3teX0FE1QGDXU08/P7y4v89EVUnDHYiIiI9wmAnIiLSIwz2FzR69GjIZDK88847KtMmT54MmUyG0aNHV35hRET0UmKwa4GTkxN27tyJ//77T2rLz89HZGQkGjRooMPKiIjoZcNg14I2bdrAyckJe/bskdr27NmDBg0awMPDQ2pTKBQIDQ2Fi4sLzM3N0apVK+zatQsAcPPmTchksjIfN2/eRHFxMcaNGyfN36xZM6xdu1aplpIjCE8/rK2tpT4LFixA69atsWXLFjg5OaFGjRoYNmwYsrOzlWpdtGgR6tevD1NTU7Ru3RrR0dHS9LLq3b9/PwDAy8sLQUFBCAoKglwuR+3atRESEgIhhDTGV199hXbt2qFmzZpwcHDAW2+9Jd0zDgDHjh2DTCbDq6++qvQa9+3bB5lMBi8vL6nNy8sLMplM6f8AADw8PCCTyXDs2DEAUGsdEhFVZwx2LRk7dizCwsKk59u2bcOYMWOU+oSGhmLHjh3YvHkz/vrrLwQHB2PkyJH47bff4OTkhLt37+Lu3bs4ffo0AOD06dNSm5OTExQKBerXr4+oqChcunQJ8+bNwwcffIDvvvtOaTl9+/aV5rt79y7WrFmjUu+1a9fw3Xff4ccff0R0dDTOnz+Pd999V5q+du1afPrpp1i5ciX+/PNP+Pj4YODAgUhKSlIa59ChQ0rL6t27tzRt+/btMDIywunTp7F27VqsWrUKX3zxhTS9qKgIixcvRkJCAvbu3YubN2+WetoiMzMTp06dkp5v2bIF9erVU+lXr149bN26VXp++vRp3Lt3T6mPuuuQiKjaEiSys7MFAJGdna0y7b///hOXLl0S//33X6nzBgYGikGDBon09HRhamoqbt68KW7evCnMzMzEvXv3xKBBg0RgYKDIz88XNWrUEH/88YfS/OPGjRP+/v5KbcnJyQKASE5Ofm7tkydPFkOHDlWp50lhYWFCLpdLz+fPny8MDQ3FP//8I7X9/PPPwsDAQNy9e1cIIYSjo6NYsmSJ0jjt27cX7777rlKN58+fL7Wu7t27Czc3N6FQKKS2OXPmCDc3tzJfy5kzZwQA8eDBAyGEEEePHhUAREhIiBg7dqwQQoi///5b2NnZiUmTJonu3bsrLW/SpEnCzs5O3Lx5UwjxeN2GhIQIAOLo0aNlLvfpdfi0520DRKQfnpUF1Qm/eU5L6tSpg/79+yM8PBxCCPTv3x+1a9eWpl+7dg0PHz5U2qMFgMLCQqXD9c+zceNGbNu2DSkpKfjvv/9QWFiI1q1ba1xvgwYNlPZ6PT09oVAocOXKFdSoUQN37txB586dlebp3LkzEhIS1F7Ga6+9pnQPuKenJz799FMUFxfD0NAQcXFxWLBgARISEvDvv/9CoVAAAFJSUtC8eXNpvsDAQHTo0AGrV6/GF198gZEjR6K4uFhleSYmJnj77bfxxRdfYNasWfj+++8RGxuLxYsXK/XT1jqkZ3N+/4CuS6CXzM1l/XVdQpXAYNeisWPHIigoCMDj8HhSbm4uAODAgQMqh5FNTU3VGn/nzp2YOXMmPv30U3h6eqJmzZr45JNPEBsbq4XqK1deXh58fHzg4+ODiIgI1KlTBykpKfDx8UFhYaFSX1tbW/j4+GDHjh3Ytm0bDh06hM2bN5c67oQJE9CzZ0/Y29ujT58+Sn9cAfq1DomISsNg16K+ffuisLAQMpkMPj4+StOaN28OU1NTpKSkoHv37uUa/+TJk+jUqZPSufDr16+Xa6yUlBTcuXMHjo6OAIBTp07BwMAAzZo1g5WVFRwdHXHy5EmlWk+ePIkOHTqovYynw/LUqVNo0qQJDA0NcfnyZdy/fx/Lli2Dk5MTAODs2bNljjVx4kQMGDAArVu3hqura5n9mjZtiiZNmuCDDz7A3r17VaZrcx0SEVVFDHYtMjQ0RGJiovTvJ9WsWRMzZ85EcHAwFAoFunTpguzsbJw8eRJWVlYIDAx87vhNmjTBjh078Msvv8DFxQVfffUVzpw5AxcXF41rNTMzQ2BgIFauXImcnBxMnToVw4YNg4ODAwBg1qxZmD9/Pho1aoTWrVsjLCwM8fHxiIiIUHsZKSkpmD59OiZOnIhz585h/fr1+PTTTwE8PhVgYmKC9evX45133sHFixdVDpk/qXv37li4cCE8PT2fu9zly5fjxIkT6NGjh9KV/oB21yERUVXEYNcyKyurMqctXrwYderUQWhoKG7cuAFra2u0adMGH3zwgVpjT5w4EefPn8fw4cMhk8ng7++Pd999Fz///LPGdTZu3Bh+fn7o168fMjMz8frrr2PTpk3S9KlTpyI7OxszZsxAeno6mjdvjh9++AFNmjRRexmjRo3Cf//9hw4dOsDQ0BDvvfceJkyYAODxNQnh4eH44IMPsG7dOrRp0wYrV67EwIEDyxwvODhYreV26NChzCML2lyHRERVkUyIJ24sfknl5ORALpcjOztbJZjz8/ORnJwMFxcXmJmZ6ahC7VqwYAH27t2L+Pj4CluGl5cXWrduXeqtdtWNPm4DlYEXz1Fle9GL556VBdUJ72MnIiLSIwx2IiIiPcJz7C+hBQsWYMGCBRW6jJKvcCUiosrFPXYiIiI9wmAnIiLSIwx2IiIiPcJgJyIi0iMMdiIiIj3CYCciItIjDHaqEDKZrNQfYSEioorF+9grWKKrW6Uuz+1yYqUur6yvp7179y5q1apVqbUQERGDnSpIya/EERFR5eKheEJ0dDS6dOkCa2tr2Nra4vXXX1f6jfJ//vkH/v7+sLGxgYWFBdq1a4fY2FiEh4dj4cKFSEhIgEwmg0wmQ3h4OADlQ/GdOnXCnDlzlJZ57949GBsb4/jx4wCAgoICzJw5E/Xq1YOFhQU6duzIb68jIioHnQb78ePHMWDAADg6OpZ5TjYxMREDBw6EXC6HhYUF2rdvj5SUFGl6fn4+Jk+eDFtbW1haWmLo0KFIS0urxFdR/eXl5WH69Ok4e/YsDh8+DAMDAwwZMgQKhQK5ubno3r07bt++jR9++AEJCQmYPXs2FAoFhg8fjhkzZqBFixa4e/cu7t69i+HDh6uMHxAQgJ07d+LJHxL89ttv4ejoiK5duwIAgoKCEBMTg507d+LPP//Em2++ib59+yIpKanS1gMRkT7Q6aH4vLw8tGrVCmPHjoWfn5/K9OvXr6NLly4YN24cFi5cCCsrK/z1119KP50ZHByMAwcOICoqCnK5HEFBQfDz88PJkycr86VUa0OHDlV6vm3bNtSpUweXLl3CH3/8gXv37uHMmTOwsbEB8Pi33EtYWlrCyMjomYfehw0bhmnTpuHEiRNSkEdGRsLf3x8ymQwpKSkICwtDSkoKHB0dAQAzZ85EdHQ0wsLCsHTpUm2/ZCIivaXTYPf19YWvr2+Z0z/88EP069cPK1askNoaNWok/Ts7OxtffvklIiMj0bNnTwBAWFgY3NzccOrUKbz22msVV7weSUpKwrx58xAbG4uMjAwoFAoAQEpKCuLj4+Hh4SGFennUqVMHffr0QUREBLp27Yrk5GTExMRgy5YtAIALFy6guLgYTZs2VZqvoKAAtra25X9hREQvoSp7jl2hUODAgQNo2rQpfHx8YGdnh44dOyodro+Li0NRURG8vb2lNldXVzRo0AAxMTFljl1QUICcnBylx8tswIAByMzMxOeff47Y2FjExsYCAAoLC2Fubq6VZQQEBGDXrl0oKipCZGQk3N3d4e7uDgDIzc2FoaEh4uLiEB8fLz0SExOxdu1arSyfiOhlUWWDPT09Hbm5uVi2bBn69u2LX3/9FUOGDIGfnx9+++03AEBqaipMTExgbW2tNK+9vT1SU1PLHDs0NBRyuVx6ODk5VeRLqdLu37+PK1eu4KOPPkKvXr3g5uaGf//9V5r+6quvIj4+HpmZmaXOb2JiguLi4ucuZ9CgQcjPz0d0dDQiIyMREBAgTfPw8EBxcTHS09PRuHFjpQevrici0kyVDfaSw8GDBg1CcHAwWrdujffffx+vv/46Nm/e/EJjz507F9nZ2dLj1q1b2ii5WqpVqxZsbW2xdetWXLt2DUeOHMH06dOl6f7+/nBwcMDgwYNx8uRJ3LhxA7t375aOiDg7OyM5ORnx8fHIyMhAQUFBqcuxsLDA4MGDERISgsTERPj7+0vTmjZtioCAAIwaNQp79uxBcnIyTp8+jdDQUBw4cKBiVwARkZ6pssFeu3ZtGBkZoXnz5krtbm5u0lXxDg4OKCwsRFZWllKftLS0Z+7pmZqawsrKSunxsjIwMMDOnTsRFxeHli1bIjg4GJ988ok03cTEBL/++ivs7OzQr18/uLu7Y9myZTA0NATw+MK7vn37okePHqhTpw6++eabMpcVEBCAhIQEdO3aFQ0aNFCaFhYWhlGjRmHGjBlo1qwZBg8ejDNnzqj0IyKiZ6uyX1BjYmKC9u3b48qVK0rtV69eRcOGDQEAbdu2hbGxMQ4fPixd2X3lyhWkpKTA09Oz0msuTWV/E1x5eHt749KlS0ptT96a1rBhQ+zatavUeU1NTUud9uT8JXx9fUttBwBjY2MsXLgQCxcu1KR0IiJ6ik6DPTc3F9euXZOelxzStbGxQYMGDTBr1iwMHz4c3bp1Q48ePRAdHY0ff/xR+uISuVyOcePGYfr06bCxsYGVlRWmTJkCT09PXhFPREQvJZ0G+9mzZ9GjRw/pecm53cDAQISHh2PIkCHYvHkzQkNDMXXqVDRr1gy7d+9Gly5dpHlWr14NAwMDDB06FAUFBfDx8cGmTZsq/bUQERFVBTJR1rHRl0hOTg7kcjmys7NVzrfn5+cjOTkZLi4uSl+MQy8PbgPl4/w+L3ykynVzWf8Xmv9ZWVCdVNmL54iIiEhzDHYiIiI9wmAnIiLSIwx2IiIiPcJgJyIi0iMMdiIiIj3CYCeNCSEwYcIE2NjYQCaTIT4+Hl5eXpg2bZrWlhEeHq7y4z5ERPR8VfYrZfVFZd/L+6L3caojOjoa4eHhOHbsGF555RXUrl1b68sYPnw4+vXrp/VxiYj0HYOdlBQWFsLExOSZfa5fv466deuiU6dOFVaHubm51n4LnojoZcJD8S85Ly8vBAUFYdq0aahduzZ8fHxw8eJF+Pr6wtLSEvb29nj77beRkZEBABg9ejSmTJmClJQUyGQyODs7lzpuQUEBZs6ciXr16sHCwgIdO3aUvuM/Pz8fLVq0wIQJE6T+169fR82aNbFt2zYAyofir169CplMhsuXLystY/Xq1WjUqJH0/Fl1ExG9LBjshO3bt8PExAQnT57EsmXL0LNnT3h4eODs2bOIjo5GWloahg0bBgBYu3YtFi1ahPr16+Pu3bs4c+ZMqWMGBQUhJiYGO3fuxJ9//ok333wTffv2RVJSEszMzBAREYHt27dj3759KC4uxsiRI9G7d2+MHTtWZaymTZuiXbt2iIiIUGqPiIjAW2+9BQDIysp6Zt1ERC8LHoonNGnSBCtWrAAAfPzxx/Dw8MDSpUul6du2bYOTkxOuXr2Kpk2bombNmjA0NCzzN+9TUlIQFhaGlJQUODo6AgBmzpyJ6OhohIWFYenSpWjdujU+/vhj/O9//8OIESPw999/Y//+/WXWGBAQgA0bNmDx4sUAHu/Fx8XF4euvvwYAbNiw4bl1ExG9DBjshLZt20r/TkhIwNGjR2FpaanS7/r162oF5IULF1BcXKzSt6CgALa2ttLzGTNmYO/evdiwYQN+/vlnpWlPGzFiBGbOnIlTp07htddeQ0REBNq0aQNXV1et1U1EpA8Y7AQLCwvp37m5uRgwYACWL1+u0q9u3bpqjZebmwtDQ0PExcXB0NBQadqTwZueno6rV6/C0NAQSUlJ6Nu3b5ljOjg4oGfPnoiMjMRrr72GyMhITJo0Sat1ExHpAwY7KWnTpg12794NZ2dnGBmVb/Pw8PBAcXEx0tPT0bVr1zL7jR07Fu7u7hg3bhzGjx8Pb29vuLm5ldk/ICAAs2fPhr+/P27cuIERI0ZotW4iIn3Ai+dIyeTJk5GZmQl/f3+cOXMG169fxy+//IIxY8aguLhYrTGaNm2KgIAAjBo1Cnv27EFycjJOnz6N0NBQHDjw+L7+jRs3IiYmBtu3b0dAQAAGDx6MgIAAFBYWljmun58fHjx4gEmTJqFHjx7S+Xtt1U1EpA8Y7KTE0dERJ0+eRHFxMfr06QN3d3dMmzYN1tbWMDBQf3MJCwvDqFGjMGPGDDRr1gyDBw/GmTNn0KBBA1y+fBmzZs3Cpk2b4OTkBADYtGkTMjIyEBISUuaYNWvWxIABA5CQkICAgIAKqZuIqLqTCSGErovQtZycHMjlcmRnZ8PKykppWn5+PpKTk+Hi4gIzMzMdVUi6xG2gfCr7WxeJXvSbN5+VBdUJd2WIiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2NXEawxfXvy/J6LqhMH+HMbGxgCAhw8f6rgS0pWS//uSbYGIqCrjV3Q9h6GhIaytrZGeng4AqFGjBmQymY6rosoghMDDhw+Rnp4Oa2trla/HJSKqihjsaij5FbOScKeXi7W1dZm/ZEdEVNUw2NUgk8lQt25d2NnZoaioSNflUCUyNjbmnjoRVSsMdg0YGhryQ56IiKo0XjxHRESkRxjsREREeoTBTkREpEcY7ERERHpEp8F+/PhxDBgwAI6OjpDJZNi7d2+Zfd955x3IZDKsWbNGqT0zMxMBAQGwsrKCtbU1xo0bh9zc3IotnIiIqIrSabDn5eWhVatW2Lhx4zP7ff/99zh16hQcHR1VpgUEBOCvv/7CwYMHsX//fhw/fhwTJkyoqJKJiIiqNJ3e7ubr6wtfX99n9rl9+zamTJmCX375Bf3791ealpiYiOjoaJw5cwbt2rUDAKxfvx79+vXDypUrS/1DgIiISJ9V6XPsCoUCb7/9NmbNmoUWLVqoTI+JiYG1tbUU6gDg7e0NAwMDxMbGljluQUEBcnJylB5ERET6oEoH+/Lly2FkZISpU6eWOj01NRV2dnZKbUZGRrCxsUFqamqZ44aGhkIul0sPJycnrdZNRESkK1U22OPi4rB27VqEh4dr/UdX5s6di+zsbOlx69YtrY5PRESkK1U22H///Xekp6ejQYMGMDIygpGREf7++2/MmDEDzs7OAB7/OMvTP8zy6NEjZGZmPvNHO0xNTWFlZaX0ICIi0gdV9rvi3377bXh7eyu1+fj44O2338aYMWMAAJ6ensjKykJcXBzatm0LADhy5AgUCgU6duxY6TUTERHpmk6DPTc3F9euXZOeJycnIz4+HjY2NmjQoAFsbW2V+hsbG8PBwQHNmjUDALi5uaFv374YP348Nm/ejKKiIgQFBWHEiBG8Ip6IiF5KOj0Uf/bsWXh4eMDDwwMAMH36dHh4eGDevHlqjxEREQFXV1f06tUL/fr1Q5cuXbB169aKKpmIiKhK0+keu5eXF4QQave/efOmSpuNjQ0iIyO1WBUREVH1VWUvniMiIiLNMdiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPRIlf099uoo0dVN1yXQS8btcqKuSyCiKoZ77ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHpEp8F+/PhxDBgwAI6OjpDJZNi7d680raioCHPmzIG7uzssLCzg6OiIUaNG4c6dO0pjZGZmIiAgAFZWVrC2tsa4ceOQm5tbya+EiIioatBpsOfl5aFVq1bYuHGjyrSHDx/i3LlzCAkJwblz57Bnzx5cuXIFAwcOVOoXEBCAv/76CwcPHsT+/ftx/PhxTJgwobJeAhERUZVipMuF+/r6wtfXt9RpcrkcBw8eVGrbsGEDOnTogJSUFDRo0ACJiYmIjo7GmTNn0K5dOwDA+vXr0a9fP6xcuRKOjo4V/hqIiIiqkmp1jj07OxsymQzW1tYAgJiYGFhbW0uhDgDe3t4wMDBAbGxsmeMUFBQgJydH6UFERKQPqk2w5+fnY86cOfD394eVlRUAIDU1FXZ2dkr9jIyMYGNjg9TU1DLHCg0NhVwulx5OTk4VWjsREVFlqRbBXlRUhGHDhkEIgc8+++yFx5s7dy6ys7Olx61bt7RQJRERke7p9By7OkpC/e+//8aRI0ekvXUAcHBwQHp6ulL/R48eITMzEw4ODmWOaWpqClNT0wqrmYiISFeq9B57SagnJSXh0KFDsLW1VZru6emJrKwsxMXFSW1HjhyBQqFAx44dK7tcIiIindPpHntubi6uXbsmPU9OTkZ8fDxsbGxQt25dvPHGGzh37hz279+P4uJi6by5jY0NTExM4Obmhr59+2L8+PHYvHkzioqKEBQUhBEjRvCKeCIieinpNNjPnj2LHj16SM+nT58OAAgMDMSCBQvwww8/AABat26tNN/Ro0fh5eUFAIiIiEBQUBB69eoFAwMDDB06FOvWrauU+omIiKoanQa7l5cXhBBlTn/WtBI2NjaIjIzUZllERETVVpU+x05ERESaYbATERHpEQY7ERGRHmGwExER6REGOxERkR5hsBMREekRjYM9Ozu7zGlffPHFCxVDREREL0bjYO/evTvu3bun1PbPP//Ax8cHISEhWiuMiIiINKdxsL/66qvo3Lmz9Iton3/+OVq0aAFbW1tcvHhR6wUSERGR+jT+5rkdO3ZgypQp6Ny5M5o1a4YLFy4gLCwMfn5+FVEfERERaaBcXym7fv16yOVyhIaG4qeffoKPj4+26yIiIqJy0DjYS36YpUOHDujZsyeGDx+OtWvXolatWgCAgQMHardCIiIiUpvGwT548GCVtjFjxgAAZDIZiouLX7goIiIiKh+Ng12hUFREHURERKQF/IIaIiIiPaLxHvu6deueOX3q1KnlLoaIiIhejMbBPm3aNNSoUQN2dnYQQihNk8lkDHYiIiId0vhQ/IcffggDAwN4e3vj1KlTSE5Olh43btyoiBqJiIhITRoH++LFi5GYmIjCwkI0a9YMS5YsQUFBQUXURkRERBoq18Vz9erVQ3h4OI4cOYLDhw+jcePG2LFjh7ZrIyIiIg1pfI79zz///P+ZjYywZs0a7Nu3D0FBQVi7di3i4uK0WiARERGpT+Ngb926NWQymXTh3JP/jo+P12pxREREpBmNgz05Obki6iAiIiIt0DjYGzZsWBF1EBERkRbwC2qIiIj0SLm+oKZ+/fowNDRUmcYvqCEiItKtcv0e+9mzZ2FnZ6ftWoiIiOgFaXwfu0wmg0wmq4haiIiI6AVpvMcuhEBISAjkcjksLCzg6OgIDw8PtG3btiLqIyIiIg1oHOzdunXD5cuXUVRUhJycHNy5cwf//vsvWrVqhQMHDsDR0bEi6iQiIiI1aBzsx44dU2m7fv06Ro0ahRkzZuCbb77RRl1ERERUDuX6rvinNWrUCGvXrsXt27e1MRwRERGVk1aCHQDatWuH48ePazTP8ePHMWDAADg6OkImk2Hv3r1K04UQmDdvHurWrQtzc3N4e3sjKSlJqU9mZiYCAgJgZWUFa2trjBs3Drm5uS/6coiIiKoljQ/F//DDD2VOk8lkGDBggNpj5eXloVWrVhg7diz8/PxUpq9YsQLr1q3D9u3b4eLigpCQEPj4+ODSpUswMzMDAAQEBODu3bs4ePAgioqKMGbMGEyYMAGRkZGavjQiIqJqT+NgHzx4cJnTZDIZiouL1R7L19cXvr6+pU4TQmDNmjX46KOPMGjQIADAjh07YG9vj71792LEiBFITExEdHQ0zpw5g3bt2gEA1q9fj379+mHlypW8kI+IiF465ToUn5qaCoVCofLQJNSfJzk5GampqfD29pba5HI5OnbsiJiYGABATEwMrK2tpVAHAG9vbxgYGCA2NrbMsQsKCpCTk6P0ICIi0gdaO8eubampqQAAe3t7pXZ7e3tpWmpqqso34BkZGcHGxkbqU5rQ0FDI5XLp4eTkpOXqiYiIdKPKBntFmjt3LrKzs6XHrVu3dF0SERGRVmh8jl0mk+HBgwfSxWtPs7KyeuGiAMDBwQEAkJaWhrp160rtaWlpaN26tdQnPT1dab5Hjx4hMzNTmr80pqamMDU11UqdREREVYnGe+xCCDRt2hS1atVSelhbW6NWrVpaK8zFxQUODg44fPiw1JaTk4PY2Fh4enoCADw9PZGVlYW4uDipz5EjR6BQKNCxY0et1UJERFRdaLzHfvToUa0tPDc3F9euXZOeJycnIz4+HjY2NmjQoAGmTZuGjz/+GE2aNJFud3N0dJSuzHdzc0Pfvn0xfvx4bN68GUVFRQgKCsKIESN4RTwREb2UNA727t27a23hZ8+eRY8ePaTn06dPBwAEBgYiPDwcs2fPRl5eHiZMmICsrCx06dIF0dHRSqcBIiIiEBQUhF69esHAwABDhw7FunXrtFYjERFRdSITQghNZ/r999+xZcsW3LhxA1FRUahXrx6++uoruLi4oEuXLhVRZ4XKycmBXC5Hdnb2C10jkOjqpsWqiJ7P7XKirksok/P7B3RdAr1kbi7r/0LzaysLdE3jc+y7d++Gj48PzM3Nce7cORQUFAAAsrOzsXTpUq0XSEREROrTONg//vhjbN68GZ9//jmMjY2l9s6dO+PcuXNaLY6IiIg0o3GwX7lyBd26dVNpl8vlyMrK0kZNREREVE4aB7uDg4PSlewlTpw4gVdeeUUrRREREVH5aBzs48ePx3vvvYfY2FjIZDLcuXMHERERmDlzJiZNmlQRNRIREZGaNL7d7f3334dCoUCvXr3w8OFDdOvWDaamppg5cyamTJlSETUSERGRmsr1lbIffvghZs2ahWvXriE3NxfNmzeHpaVlRdRHREREGtA42EuYmJigZs2aqFmzJkOdiIioitD4HPujR48QEhICuVwOZ2dnODs7Qy6X46OPPkJRUVFF1EhERERq0niPfcqUKdizZw9WrFgh/RhLTEwMFixYgPv37+Ozzz7TepFERESkHo2DPTIyEjt37oSvr6/U9uqrr8LJyQn+/v4MdiIiIh3S+FC8qakpnJ2dVdpdXFxgYmKijZqIiIionDQO9qCgICxevFj6jngAKCgowJIlSxAUFKTV4oiIiEgzGh+KP3/+PA4fPoz69eujVatWAICEhAQUFhaiV69e8PPzk/ru2bNHe5USERHRc2kc7NbW1hg6dKhSm5OTk9YKIiIiovLTONjDwsIqog4iIiLSAo3PsRMREVHVxWAnIiLSIwx2IiIiPcJgJyIi0iMMdiIiIj1SrmD/7bffMGDAADRu3BiNGzfGwIED8fvvv2u7NiIiItKQxsH+9ddfw9vbGzVq1MDUqVMxdepUmJubo1evXoiMjKyIGomIiEhNGt/HvmTJEqxYsQLBwcFS29SpU7Fq1SosXrwYb731llYLJCIiIvVpvMd+48YNDBgwQKV94MCBSE5O1kpRREREVD4aB7uTkxMOHz6s0n7o0CF+tSwREZGOaXwofsaMGZg6dSri4+PRqVMnAMDJkycRHh6OtWvXar1AIiIiUp/GwT5p0iQ4ODjg008/xXfffQcAcHNzw7fffotBgwZpvUAiIiJSn8bBDgBDhgzBkCFDtF0LERERvaBy3cf+8OFDFBQUAABSUlKwbds2/PHHH1otjIiIiDRXrvvY5XI5HBwc8MMPP8Dd3R1z5sxBt27dsHnz5oqokYiIiNSkcbAvWbIEwcHBmDdvHkaOHImFCxfi3r172Lx5M1avXl0RNRIREZGaynUf+5QpUzBt2jQUFBTA19cXAODr64ubN29qtbji4mKEhITAxcUF5ubmaNSoERYvXgwhhNRHCIF58+ahbt26MDc3h7e3N5KSkrRaBxERUXWhcbAXFRXBzMwMMpkMJiYmMDExAQAYGRnh0aNHWi1u+fLl+Oyzz7BhwwYkJiZi+fLlWLFiBdavXy/1WbFiBdatW4fNmzcjNjYWFhYW8PHxQX5+vlZrISIiqg7KdVX86NGjYWpqivz8fLzzzjuwsLCQLqbTpj/++AODBg1C//79AQDOzs745ptvcPr0aQCP99bXrFmDjz76SLrVbseOHbC3t8fevXsxYsQIrddERERUlWm8xx4YGAg7OzvI5XKMHDkSjo6OkMvlsLOzw6hRo7RaXKdOnXD48GFcvXoVAJCQkIATJ05Ih/+Tk5ORmpoKb29vaR65XI6OHTsiJiamzHELCgqQk5Oj9CAiItIHGu+xh4WFVUQdpXr//feRk5MDV1dXGBoaori4GEuWLEFAQAAAIDU1FQBgb2+vNJ+9vb00rTShoaFYuHBhxRVORESkI+W6j73EsmXLkJWVpaVSVH333XeIiIhAZGQkzp07h+3bt2PlypXYvn37C407d+5cZGdnS49bt25pqWIiIiLdeqFgX7p0KTIzM7VVi4pZs2bh/fffx4gRI+Du7o63334bwcHBCA0NBQA4ODgAANLS0pTmS0tLk6aVxtTUFFZWVkoPIiIiffBCwf7kbWcV4eHDhzAwUC7R0NAQCoUCAODi4gIHBwelX5vLyclBbGwsPD09K7Q2IiKiqqhcV8VXlgEDBmDJkiVo0KABWrRogfPnz2PVqlUYO3YsAEAmk2HatGn4+OOP0aRJE7i4uCAkJASOjo4YPHiwbosnIiLSgRcK9kuXLsHR0VFbtahYv349QkJC8O677yI9PR2Ojo6YOHEi5s2bJ/WZPXs28vLyMGHCBGRlZaFLly6Ijo6GmZlZhdVFRERUVcmEhsfTn3drWHU8X52TkwO5XI7s7OwXqj/R1U2LVRE9n9vlRF2XUCbn9w/ougR6ydxc1v+F5tdWFuiaxnvs1tbWkMlkKu1CCMhkMhQXF2ulMCIiItJcuQ7F79q1CzY2NtquhYiIiF5QuYK9c+fOsLOz03YtRERE9ILKFeyXLl3C/fv3YWFhAQcHB+mHYIiIiEi3ynUfe69evdCiRQu4uLjAwsIC7u7u/C12IiKiKkDjPfbk5GQIIVBUVIScnBzcuXMHp0+fRkhICB49eoRZs2ZVRJ1ERESkBo2DvWHDhkrP27ZtiwEDBqBp06ZYtGgRg52IiEiHtPbNcyNGjECLFi20NRwRERGVQ7mDPS4uDomJj78co3nz5mjTpg3atGmjtcKIiIhIcxoHe3p6OkaMGIFjx47B2toaAJCVlYUePXpg586dqFOnjrZrJCIiIjVpfFX8lClT8ODBA/z111/IzMxEZmYmLl68iJycHEydOrUiaiQiIiI1abzHHh0djUOHDsHN7f+/F7158+bYuHEj+vTpo9XiiIiISDMa77ErFAoYGxurtBsbG0u/k05ERES6oXGw9+zZE++99x7u3Lkjtd2+fRvBwcHo1auXVosjIiIizWgc7Bs2bEBOTg6cnZ3RqFEjNGrUCC4uLsjJycH69esrokYiIiJSk8bn2J2cnHDu3DkcOnQIly9fBgC4ubnB29tb68URERGRZtQO9gcPHqBmzZoAAJlMht69e6N3795Kfc6cOYP27dtrt0IiIiJSm9qH4vv06YPc3NxSpz169AgfffQROnfurLXCiIiISHNqB/uDBw/g7e2NnJwcpfaLFy+iffv2CA8Px969e7VdHxEREWlA7WA/evQo8vLy0Lt3b+Tk5EAIgeXLl6Ndu3Zwc3PDhQsX0K9fv4qslYiIiJ5D7XPsderUwZEjR+Dt7Y2ePXvC1NQUSUlJ+Prrr/HGG29UZI1ERESkJo2uiq9Tpw4OHz4Mb29vXLx4EfHx8XB1da2o2oiIiEhDGt/HXrt2bRw5cgTNmzfHW2+9hX///bci6iIiIqJyUHuP3c/PT+m5lZUVjh8/jg4dOsDd3V1q37Nnj/aqIyIiIo2oHexyuVzluYuLi9YLIiIiovJTO9jDwsIqsg4iIiLSAo3PsRMREVHVxWAnIiLSIwx2IiIiPcJgJyIi0iMMdiIiIj1S5YP99u3bGDlyJGxtbWFubg53d3ecPXtWmi6EwLx581C3bl2Ym5vD29sbSUlJOqyYiIhId6p0sP/777/o3LkzjI2N8fPPP+PSpUv49NNPUatWLanPihUrsG7dOmzevBmxsbGwsLCAj48P8vPzdVg5ERGRbmj0XfGVbfny5XByclK6h/7JL8URQmDNmjX46KOPMGjQIADAjh07YG9vj71792LEiBGVXjMREZEuVek99h9++AHt2rXDm2++CTs7O3h4eODzzz+XpicnJyM1NRXe3t5Sm1wuR8eOHRETE6OLkomIiHSqSgf7jRs38Nlnn6FJkyb45ZdfMGnSJEydOhXbt28HAKSmpgIA7O3tleazt7eXppWmoKAAOTk5Sg8iIiJ9UKUPxSsUCrRr1w5Lly4FAHh4eODixYvYvHkzAgMDyz1uaGgoFi5cqK0yiYiIqowqvcdet25dNG/eXKnNzc0NKSkpAAAHBwcAQFpamlKftLQ0aVpp5s6di+zsbOlx69YtLVdORESkG1U62Dt37owrV64otV29ehUNGzYE8PhCOgcHBxw+fFianpOTg9jYWHh6epY5rqmpKaysrJQeRERE+qBKH4oPDg5Gp06dsHTpUgwbNgynT5/G1q1bsXXrVgCATCbDtGnT8PHHH6NJkyZwcXFBSEgIHB0dMXjwYN0WT0REpANVOtjbt2+P77//HnPnzsWiRYvg4uKCNWvWICAgQOoze/Zs5OXlYcKECcjKykKXLl0QHR0NMzMzHVZORESkGzIhhNB1EbqWk5MDuVyO7OzsFzosn+jqpsWqiJ7P7XKirksok/P7B3RdAr1kbi7r/0LzaysLdK1Kn2MnIiIizTDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9Ei1CvZly5ZBJpNh2rRpUlt+fj4mT54MW1tbWFpaYujQoUhLS9NdkURERDpUbYL9zJkz2LJlC1599VWl9uDgYPz444+IiorCb7/9hjt37sDPz09HVRIREelWtQj23NxcBAQE4PPPP0etWrWk9uzsbHz55ZdYtWoVevbsibZt2yIsLAx//PEHTp06pcOKiYiIdKNaBPvkyZPRv39/eHt7K7XHxcWhqKhIqd3V1RUNGjRATExMZZdJRESkc0a6LuB5du7ciXPnzuHMmTMq01JTU2FiYgJra2uldnt7e6SmppY5ZkFBAQoKCqTnOTk5WquXiIhIl6r0HvutW7fw3nvvISIiAmZmZlobNzQ0FHK5XHo4OTlpbWwiIiJdqtLBHhcXh/T0dLRp0wZGRkYwMjLCb7/9hnXr1sHIyAj29vYoLCxEVlaW0nxpaWlwcHAoc9y5c+ciOztbety6dauCXwkREVHlqNKH4nv16oULFy4otY0ZMwaurq6YM2cOnJycYGxsjMOHD2Po0KEAgCtXriAlJQWenp5ljmtqagpTU9MKrZ2IiEgXqnSw16xZEy1btlRqs7CwgK2trdQ+btw4TJ8+HTY2NrCyssKUKVPg6emJ1157TRclExER6VSVDnZ1rF69GgYGBhg6dCgKCgrg4+ODTZs26bosIiIinah2wX7s2DGl52ZmZti4cSM2btyom4KIiIiqkCp98RwRERFphsFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkR6p8sIeGhqJ9+/aoWbMm7OzsMHjwYFy5ckWpT35+PiZPngxbW1tYWlpi6NChSEtL01HFREREulPlg/23337D5MmTcerUKRw8eBBFRUXo06cP8vLypD7BwcH48ccfERUVhd9++w137tyBn5+fDqsmIiLSDSNdF/A80dHRSs/Dw8NhZ2eHuLg4dOvWDdnZ2fjyyy8RGRmJnj17AgDCwsLg5uaGU6dO4bXXXtNF2URERDpR5ffYn5adnQ0AsLGxAQDExcWhqKgI3t7eUh9XV1c0aNAAMTExpY5RUFCAnJwcpQcREZE+qFbBrlAoMG3aNHTu3BktW7YEAKSmpsLExATW1tZKfe3t7ZGamlrqOKGhoZDL5dLDycmpoksnIiKqFNUq2CdPnoyLFy9i586dLzTO3LlzkZ2dLT1u3bqlpQqJiIh0q8qfYy8RFBSE/fv34/jx46hfv77U7uDggMLCQmRlZSnttaelpcHBwaHUsUxNTWFqalrRJRMREVW6Kr/HLoRAUFAQvv/+exw5cgQuLi5K09u2bQtjY2McPnxYarty5QpSUlLg6elZ2eUSERHpVJXfY588eTIiIyOxb98+1KxZUzpvLpfLYW5uDrlcjnHjxmH69OmwsbGBlZUVpkyZAk9PT14RT0REL50qH+yfffYZAMDLy0upPSwsDKNHjwYArF69GgYGBhg6dCgKCgrg4+ODTZs2VXKlREREulflg10I8dw+ZmZm2LhxIzZu3FgJFREREVVdVf4cOxEREamPwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRHGOxERER6hMFORESkRxjsREREeoTBTkREpEcY7ERERHqEwU5ERKRH9CbYN27cCGdnZ5iZmaFjx444ffq0rksiIiKqdHoR7N9++y2mT5+O+fPn49y5c2jVqhV8fHyQnp6u69KIiIgqlV4E+6pVqzB+/HiMGTMGzZs3x+bNm1GjRg1s27ZN16URERFVKiNdF/CiCgsLERcXh7lz50ptBgYG8Pb2RkxMTKnzFBQUoKCgQHqenZ0NAMjJyXmhWnKLi19ofiJNveg2W5EUBQ91XQK9ZF70/VAyvxBCG+XoTLUP9oyMDBQXF8Pe3l6p3d7eHpcvXy51ntDQUCxcuFCl3cnJqUJqJKowcrmuKyCqMuRrtDPOgwcPIK/G761qH+zlMXfuXEyfPl16rlAokJmZCVtbW8hkMh1W9nLKycmBk5MTbt26BSsrK12XQ6QzfC/olhACDx48gKOjo65LeSHVPthr164NQ0NDpKWlKbWnpaXBwcGh1HlMTU1hamqq1GZtbV1RJZKarKys+GFGBL4XdKk676mXqPYXz5mYmKBt27Y4fPiw1KZQKHD48GF4enrqsDIiIqLKV+332AFg+vTpCAwMRLt27dChQwesWbMGeXl5GDNmjK5LIyIiqlR6EezDhw/HvXv3MG/ePKSmpqJ169aIjo5WuaCOqiZTU1PMnz9f5fQI0cuG7wXSBpmo7tf1ExERkaTan2MnIiKi/8dgJyIi0iMMdiIiIj3CYKcqQyaTYe/evboug+iFCSEwYcIE2NjYQCaTIT4+Hl5eXpg2bZrWlhEeHs7v36BSMdip0i1YsACtW7dWab979y58fX0rvyAiLYuOjkZ4eDj279+Pu3fvomXLllpfxvDhw3H16lWtj0vVn17c7kb6oaxvCiSqSgoLC2FiYvLMPtevX0fdunXRqVOnCqvD3Nwc5ubmFTY+VV/cY6dyiY6ORpcuXWBtbQ1bW1u8/vrruH79ujT9n3/+gb+/P2xsbGBhYYF27dohNjYW4eHhWLhwIRISEiCTySCTyRAeHg5A+VB8p06dMGfOHKVl3rt3D8bGxjh+/DiAx7/SN3PmTNSrVw8WFhbo2LEjjh07Vhkvn14iXl5eCAoKwrRp01C7dm34+Pjg4sWL8PX1haWlJezt7fH2228jIyMDADB69GhMmTIFKSkpkMlkcHZ2LnXcZ22/+fn5aNGiBSZMmCD1v379OmrWrCn9HPWTh+KvXr0KmUym8sNXq1evRqNGjaTnz6qb9AeDncolLy8P06dPx9mzZ3H48GEYGBhgyJAhUCgUyM3NRffu3XH79m388MMPSEhIwOzZs6FQKDB8+HDMmDEDLVq0wN27d3H37l0MHz5cZfyAgADs3LlT6ecTv/32Wzg6OqJr164AgKCgIMTExGDnzp34888/8eabb6Jv375ISkqqtPVAL4ft27fDxMQEJ0+exLJly9CzZ094eHjg7NmziI6ORlpaGoYNGwYAWLt2LRYtWoT69evj7t27OHPmTKljPmv7NTMzQ0REBLZv3459+/ahuLgYI0eORO/evTF27FiVsZo2bYp27dohIiJCqT0iIgJvvfUWACArK+uZdZMeEURacO/ePQFAXLhwQWzZskXUrFlT3L9/v9S+8+fPF61atVJpByC+//57IYQQ6enpwsjISBw/flya7unpKebMmSOEEOLvv/8WhoaG4vbt20pj9OrVS8ydO1c7L4pICNG9e3fh4eEhPV+8eLHo06ePUp9bt24JAOLKlStCCCFWr14tGjZsqDLOe++9J4RQf/tdsWKFqF27tggKChJ169YVGRkZ0rSwsDAhl8ul56tXrxaNGjWSnl+5ckUAEImJiWrXTfqB59ipXJKSkjBv3jzExsYiIyMDCoUCAJCSkoL4+Hh4eHjAxsam3OPXqVMHffr0QUREBLp27Yrk5GTExMRgy5YtAIALFy6guLgYTZs2VZqvoKAAtra25X9hRKVo27at9O+EhAQcPXoUlpaWKv2uX7+usk2WRt3td8aMGdi7dy82bNiAn3/++Znb9ogRIzBz5kycOnUKr732GiIiItCmTRu4urpqrW6qHhjsVC4DBgxAw4YN8fnnn8PR0REKhQItW7ZEYWGh1i7oCQgIwNSpU7F+/XpERkbC3d0d7u7uAIDc3FwYGhoiLi4OhoaGSvOV9sFF9CIsLCykf+fm5mLAgAFYvny5Sr+6deuqNZ662296ejquXr0KQ0NDJCUloW/fvmWO6eDggJ49eyIyMhKvvfYaIiMjMWnSJK3WTdUDg500dv/+fVy5cgWff/65dL77xIkT0vRXX30VX3zxBTIzM0vdazcxMUFxcfFzlzNo0CBMmDAB0dHRiIyMxKhRo6RpHh4eKC4uRnp6ulQDUWVo06YNdu/eDWdnZxgZle8jVN3td+zYsXB3d8e4ceMwfvx4eHt7w83Nrcz+AQEBmD17Nvz9/XHjxg2MGDFCq3VT9cCL50hjtWrVgq2tLbZu3Ypr167hyJEjmD59ujTd398fDg4OGDx4ME6ePIkbN25g9+7diImJAQA4OzsjOTkZ8fHxyMjIQEFBQanLsbCwwODBgxESEoLExET4+/tL05o2bYqAgACMGjUKe/bsQXJyMk6fPo3Q0FAcOHCgYlcAvdQmT56MzMxM+Pv748yZM7h+/Tp++eUXjBkzRq0/WAH1tt+NGzciJiYG27dvR0BAAAYPHoyAgAAUFhaWOa6fnx8ePHiASZMmoUePHnB0dNRq3VQ9MNhJYwYGBti5cyfi4uLQsmVLBAcH45NPPpGmm5iY4Ndff4WdnR369esHd3d3LFu2TDrkOHToUPTt2xc9evRAnTp18M0335S5rICAACQkJKBr165o0KCB0rSwsDCMGjUKM2bMQLNmzTB48GCcOXNGpR+RNjk6OuLkyZMoLi5Gnz594O7ujmnTpsHa2hoGBup/pD5r+718+TJmzZqFTZs2wcnJCQCwadMmZGRkICQkpMwxa9asiQEDBiAhIQEBAQEVUjdVffzZViIiIj3CP9OIiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiPMNiJiIj0CIOdiIhIjzDYiYiI9AiDnYiISI8w2ImIiPQIg52IiEiP/B/7X79ulAczYQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fruits = ['active', 'reflexive']\n",
    "counts = [131, 171]\n",
    "bar_labels = ['active', 'reflexive']\n",
    "bar_colors = ['tab:red', 'tab:blue']\n",
    "\n",
    "ax.bar(fruits, counts, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('Кол-во реплик')\n",
    "ax.set_title('Распределение аннотированных данных по лейблам')\n",
    "ax.legend(title='Метапрограмма')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737b68c-5551-4ae8-92ba-504064eb0bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
